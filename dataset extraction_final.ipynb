{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the subset of fake/real articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "#The file uploaded should be a subset of the full dataset consisting of only fake and real articles. \n",
    "dataset = pd.read_csv('C:/Users/Irisw/fake_reliable.csv', engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.domain.value_counts()\n",
    "\n",
    "fake = dataset[dataset['type']==\"fake\"]\n",
    "real = dataset[dataset['type']==\"reliable\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ending(string, removed):\n",
    "    if removed in string:\n",
    "        i = string.index(removed)\n",
    "        return string[:i]\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "def remove_start(string, removed):\n",
    "    if removed in string:\n",
    "        i = string.index(removed)\n",
    "        return string[i:]\n",
    "    else:\n",
    "        return string\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGazette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgazette = fake[fake['domain'] == 'dcgazette.com']\n",
    "dcgazette = dcgazette.reset_index(drop=True)\n",
    "dcgazette = dcgazette.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgazette = dcgazette.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "dcgazette = dcgazette.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "dcgazette = dcgazette.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "dcgazette['Archive'] = ['Archive' in title for title in dcgazette['title']]\n",
    "dcgazette['authorurl'] = ['/author' in url for url in dcgazette['url']]\n",
    "dcgazette['termsconditions'] = ['terms-and-conditions' in url for url in dcgazette['url']]\n",
    "\n",
    "dcgazette = dcgazette[dcgazette['Archive'] == False]\n",
    "dcgazette = dcgazette[dcgazette['authorurl'] == False]\n",
    "dcgazette = dcgazette[dcgazette['termsconditions'] == False]\n",
    "\n",
    "dcgazette = dcgazette.reset_index(drop=True)\n",
    "\n",
    "dcgazette = dcgazette[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Truth Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truthdivision = fake[fake['domain'] == 'thetruthdivision.com']\n",
    "Truthdivision = Truthdivision.reset_index(drop=True)\n",
    "Truthdivision = Truthdivision.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Truthdivision = Truthdivision.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "Truthdivision = Truthdivision.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "Truthdivision = Truthdivision.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "Truthdivision['Archive'] = ['Archive' in title for title in Truthdivision['title']]\n",
    "Truthdivision['truthdiv'] = ['The Truth Division' in title for title in Truthdivision['title']]\n",
    "Truthdivision['authorurl'] = ['/author' in url for url in Truthdivision['url']]\n",
    "\n",
    "\n",
    "Truthdivision = Truthdivision[Truthdivision['Archive'] == False]\n",
    "Truthdivision = Truthdivision[Truthdivision['truthdiv'] == False]\n",
    "Truthdivision = Truthdivision[Truthdivision['authorurl'] == False]\n",
    "\n",
    "Truthdivision = Truthdivision.reset_index(drop=True)\n",
    "Truthdivision = Truthdivision[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clashdaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clashdaily = fake[fake['domain'] == 'clashdaily.com']\n",
    "clashdaily = clashdaily.reset_index(drop=True)\n",
    "clashdaily = clashdaily.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clashdaily = clashdaily.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "clashdaily = clashdaily.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "clashdaily = clashdaily.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "clashdaily['authorurl'] = ['/author' in url for url in clashdaily['url']]\n",
    "clashdaily['Abouttheauthor'] = ['About the author' in content for content in clashdaily['content']]\n",
    "clashdaily['shirt'] = ['This shirt is made in the USA' in content for content in clashdaily['content']]\n",
    "clashdaily['disclm'] = ['We have no tolerance for comments containing violence, racism, vulgarity, profanity, all caps' in content for content in clashdaily['content']]\n",
    "\n",
    "clashdaily = clashdaily[clashdaily['Abouttheauthor'] == False]\n",
    "clashdaily = clashdaily[clashdaily['shirt'] == False]\n",
    "clashdaily = clashdaily[clashdaily['disclm'] == False]\n",
    "clashdaily = clashdaily[clashdaily['authorurl'] == False]\n",
    "\n",
    "clashdaily['content'] = clashdaily['content'].apply(func = shorten, args = [\"The Effeminization Of The American Male\"])\n",
    "clashdaily = clashdaily.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clashdaily = clashdaily[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conservativedailypost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conservativedailypost = fake[(fake['domain'] =='conservativedailypost.com')]\n",
    "conservativedailypost = conservativedailypost.dropna(subset=['content', 'title'])\n",
    "conservativedailypost = conservativedailypost.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conservativedailypost = conservativedailypost.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "conservativedailypost = conservativedailypost.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "conservativedailypost = conservativedailypost.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "conservativedailypost['Author'] = ['/author' in url for url in conservativedailypost['url']]\n",
    "conservativedailypost = conservativedailypost[conservativedailypost['Author'] == False]\n",
    "\n",
    "conservativedailypost = conservativedailypost[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freedomdaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freedomdaily = fake[fake['domain'] == 'freedomdaily.com']\n",
    "freedomdaily = freedomdaily.dropna(subset=['content', 'title'])\n",
    "freedomdaily = freedomdaily.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freedomdaily = freedomdaily.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "freedomdaily = freedomdaily.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "freedomdaily = freedomdaily.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "freedomdaily['Archive'] = ['Archive' in title for title in freedomdaily['title']]\n",
    "freedomdaily = freedomdaily[freedomdaily['Archive'] == False]\n",
    "\n",
    "freedomdaily['error'] = ['This field is for validation purposes and should be left unchanged.' in content for content in freedomdaily['content']]\n",
    "freedomdaily = freedomdaily[freedomdaily['error'] == False]\n",
    "\n",
    "freedomdaily = freedomdaily.reset_index(drop=True)\n",
    "freedomdaily.drop(freedomdaily.index[22], inplace=False)\n",
    "freedomdaily = freedomdaily[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seventynews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventynews = fake[fake['domain'] == '70news.wordpress.com']\n",
    "seventynews = seventynews.dropna(subset=['content', 'title'])\n",
    "seventynews = seventynews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventynews = seventynews.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "seventynews = seventynews.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "seventynews = seventynews.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "seventynews['« 70news'] = ['« 70news' in title for title in seventynews['title']]\n",
    "seventynews = seventynews[seventynews['« 70news'] == False]\n",
    "seventynews = seventynews.reset_index(drop=True)\n",
    "seventynews = seventynews[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Free Patriot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefreepatriot = fake[fake['domain'] == 'thefreepatriot.org']\n",
    "thefreepatriot = thefreepatriot.dropna(subset=['content', 'title'])\n",
    "thefreepatriot = thefreepatriot.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefreepatriot = thefreepatriot.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "thefreepatriot = thefreepatriot.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "thefreepatriot = thefreepatriot.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "thefreepatriot['notarticle'] = ['– The Free Patriot' in title for title in thefreepatriot['title']]\n",
    "thefreepatriot = thefreepatriot[thefreepatriot['notarticle'] == False]\n",
    "\n",
    "thefreepatriot['photos'] = ['Photos:' in title for title in thefreepatriot['title']]\n",
    "thefreepatriot = thefreepatriot[thefreepatriot['photos'] == False]\n",
    "\n",
    "thefreepatriot['author'] = ['/author' in title for title in thefreepatriot['url']]\n",
    "thefreepatriot = thefreepatriot[thefreepatriot['author'] == False]\n",
    "\n",
    "thefreepatriot['error'] = ['(function()' in content for content in thefreepatriot['content']]\n",
    "thefreepatriot = thefreepatriot[thefreepatriot['error'] == False]\n",
    "\n",
    "thefreepatriot = thefreepatriot.reset_index(drop=True)\n",
    "thefreepatriot = thefreepatriot[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA Supreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usasupreme = fake[fake['domain'] == 'usasupreme.com']\n",
    "usasupreme = usasupreme.dropna(subset=['content', 'title'])\n",
    "usasupreme = usasupreme.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usasupreme = usasupreme.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "usasupreme = usasupreme.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "usasupreme = usasupreme.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usasupreme['Archive'] = ['Archive' in title for title in usasupreme['title']]\n",
    "usasupreme = usasupreme[usasupreme['Archive'] == False]\n",
    "\n",
    "usasupreme = usasupreme.reset_index(drop=True)\n",
    "usasupreme = usasupreme[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaparty.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaparty = fake[fake['domain'] == 'teaparty.org']\n",
    "teaparty = teaparty.dropna(subset=['content', 'title'])\n",
    "teaparty = teaparty.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaparty = teaparty.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "teaparty = teaparty.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "teaparty = teaparty.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaparty['Archive'] = ['Archive' in title for title in teaparty['title']]\n",
    "teaparty = teaparty[teaparty['Archive'] == False]\n",
    "\n",
    "teaparty['corsi'] = ['Corsi:' in title for title in teaparty['title']]\n",
    "teaparty = teaparty[teaparty['corsi'] == False]\n",
    "\n",
    "teaparty = teaparty.reset_index(drop=True)\n",
    "teaparty = teaparty[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL SOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = real[real['domain'] == 'www.reuters.com']\n",
    "reuters = reuters.dropna(subset=['content', 'title'])\n",
    "reuters = reuters.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = reuters.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "reuters = reuters.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "reuters = reuters.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters['video'] = ['video' in url for url in reuters['url']]\n",
    "reuters = reuters[reuters['video'] == False]\n",
    "\n",
    "reuters['Page'] = ['Page' in title for title in reuters['title']]\n",
    "reuters = reuters[reuters['Page'] == False]\n",
    "\n",
    "reuters['author'] = ['/author' in url for url in reuters['url']]\n",
    "reuters = reuters[reuters['author'] == False]\n",
    "\n",
    "reuters['category'] = ['category' in url for url in reuters['url']]\n",
    "reuters = reuters[reuters['category'] == False]\n",
    "\n",
    "reuters['Brief'] = ['BRIEF' in title for title in reuters['title']]\n",
    "reuters = reuters[reuters['Brief'] == False]\n",
    "\n",
    "reuters['tag'] = ['tag' in url for url in reuters['url']]\n",
    "reuters = reuters[reuters['tag'] == False]\n",
    "\n",
    "reuters['pol'] = ['Politics |' in url for url in reuters['content']]\n",
    "reuterspol = reuters[reuters['pol'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = reuterspol[['content', 'title', 'type', 'url', 'domain']]\n",
    "reuters = reuters.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Houston Cronicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron = real[real['domain'] == 'www.chron.com']\n",
    "chron = chron.dropna(subset=['content', 'title'])\n",
    "chron = chron.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron = chron.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "chron = chron.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "chron = chron.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron['slideshow'] = ['/slideshow' in url for url in chron['url']]\n",
    "chron = chron[chron['slideshow'] == False]\n",
    "\n",
    "chron['urlpol'] = ['http://www.chron.com/news/politics' in url for url in chron['url']]\n",
    "politics = chron[chron['urlpol'] == True]\n",
    "\n",
    "politics['content'] = politics['content'].apply(func = remove_start, args = [\"(AP) —\"])\n",
    "\n",
    "politics = politics.reset_index(drop=True)\n",
    "chron = politics[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian = real[real['domain'] == 'www.theguardian.com']\n",
    "guardian = guardian.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian = guardian.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "guardian = guardian.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "guardian = guardian.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import options\n",
    "options.display.max_colwidth = 100\n",
    "\n",
    "guardian['news'] = ['us-news' in url for url in guardian['url']]\n",
    "guardiannews = guardian[guardian['news'] == True]\n",
    "\n",
    "guardian['politics'] = ['/politics' in url for url in guardian['url']]\n",
    "guardianpolitics = guardian[guardian['politics'] == True]\n",
    "\n",
    "guardian = guardianpolitics.append(guardiannews)\n",
    "\n",
    "guardian['/video'] = ['/video' in url for url in guardian['url']]\n",
    "guardian = guardian[guardian['/video'] == False]\n",
    "\n",
    "guardian = guardian[['content', 'title', 'type', 'url', 'domain']]\n",
    "guardian = guardian.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abcnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = real[real['domain'] == 'abcnews.go.com']\n",
    "abc = abc.dropna(subset=['content', 'title'])\n",
    "abc = abc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = abc.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "abc = abc.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "abc = abc.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc['politics'] = ['/Politics' in url for url in abc['url']]\n",
    "\n",
    "abcpolitics = abc[abc['politics'] == True]\n",
    "\n",
    "abcpolitics['transcript'] = ['Video Transcript' in url for url in abcpolitics['content']]\n",
    "abcpolitics = abcpolitics[abcpolitics['transcript'] == False]\n",
    "\n",
    "abcpolitics['/video'] = ['/video' in url for url in abcpolitics['url']]\n",
    "abcpolitics = abcpolitics[abcpolitics['/video'] == False]\n",
    "\n",
    "\n",
    "abcpolitics['PHOTO:'] = ['/video' in title for title in abcpolitics['title']]\n",
    "abcpolitics = abcpolitics[abcpolitics['/video'] == False]\n",
    "\n",
    "abcpolitics = abcpolitics.reset_index(drop=True)\n",
    "\n",
    "abcpolitics = abcpolitics[['content', 'title', 'type', 'url', 'domain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloomberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloomberg = real[real['domain'] == 'www.bloomberg.com']\n",
    "bloomberg = bloomberg.dropna(subset=['content', 'title'])\n",
    "bloomberg = bloomberg.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bloomberg = bloomberg.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "bloomberg = bloomberg.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "bloomberg = bloomberg.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "bloomberg['/videos'] = ['/video' in url for url in bloomberg['url']]\n",
    "bloomberg = bloomberg[bloomberg['/videos'] == False]\n",
    "\n",
    "bloomberg['politics'] = ['/politics' in url for url in bloomberg['url']]\n",
    "bloompolitics = bloomberg[bloomberg['politics'] == True]\n",
    "\n",
    "bloomberg['news'] = ['/news/articles' in url for url in bloomberg['url']]\n",
    "bloomnews = bloomberg[bloomberg['news'] == True]\n",
    "\n",
    "bloompolitics = bloompolitics[['content', 'title', 'type', 'url', 'domain']]\n",
    "bloompolitics = bloompolitics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NyTimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes = real[real['domain'] == 'www.nytimes.com']\n",
    "nytimes = nytimes.dropna(subset=['content', 'title'])\n",
    "nytimes = nytimes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes = nytimes.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "nytimes = nytimes.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "nytimes = nytimes.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "nytimes['politics'] = ['/politics' in url for url in nytimes['url']]\n",
    "nytimespol = nytimes[nytimes['politics'] == True]\n",
    "\n",
    "nytimespol['video'] = ['/video' in url for url in nytimespol['url']]\n",
    "nytimespol = nytimespol[nytimespol['video'] == False]\n",
    "\n",
    "nytimespol = nytimespol[['content', 'title', 'type', 'url', 'domain']]\n",
    "nytimespol = nytimespol.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFgate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.display.max_colwidth = 100\n",
    "\n",
    "sfgate = real[real['domain'] == 'www.sfgate.com']\n",
    "sfgate = sfgate.dropna(subset=['content', 'title'])\n",
    "sfgate = sfgate.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfgate = sfgate.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "sfgate = sfgate.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "sfgate = sfgate.drop_duplicates(subset= ['url'], keep = 'first')\n",
    "\n",
    "sfgate['politics'] = ['news/politics' in url for url in sfgate['url']]\n",
    "sfgate = sfgate[sfgate['politics'] == True]\n",
    "\n",
    "sfgate['content'] = sfgate['content'].apply(func = remove_start, args = [\"(AP) —\"])\n",
    "\n",
    "\n",
    "sfgate = sfgate[['content', 'title', 'type', 'url', 'domain']]\n",
    "sfgate = sfgate.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politico = real[real['domain'] == 'www.politico.com']\n",
    "politico = politico.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politico['story'] = ['/story' in url for url in politico['url']]\n",
    "politico = politico[politico['story'] == True]\n",
    "\n",
    "politico['nerdcast'] = ['nerdcast' in url for url in politico['url']]\n",
    "politico = politico[politico['nerdcast'] == False]\n",
    "\n",
    "politico = politico[['content', 'title', 'type', 'url', 'domain']]\n",
    "politico = politico.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USAtoday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usatoday = real[real['domain'] == 'www.usatoday.com']\n",
    "usatoday = usatoday.dropna(subset=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usatoday['story'] = ['/story/news/politics' in url for url in usatoday['url']]\n",
    "usatoday = usatoday[usatoday['story'] == True]\n",
    "\n",
    "usatoday = usatoday[['content', 'title', 'type', 'url', 'domain']]\n",
    "usatoday = usatoday.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append all the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeselected = dcgazette.append([Truthdivision, clashdaily, conservativedailypost, freedomdaily, seventynews, thefreepatriot, usasupreme, teaparty])\n",
    "fakeselected = fakeselected.reset_index(drop=True)\n",
    "\n",
    "realselected = chron.append([reuters, guardian, abcpolitics, bloompolitics, nytimespol, sfgate, politico, usatoday]) \n",
    "realselected = realselected.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realselected = realselected.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "realselected = realselected.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "realselected = realselected.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeselected = fakeselected.drop_duplicates(subset= ['title'], keep = 'first')\n",
    "fakeselected = fakeselected.drop_duplicates(subset= ['content'], keep = 'first')\n",
    "fakeselected = fakeselected.drop_duplicates(subset= ['url'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_subset = fakeselected.append(realselected)\n",
    "complete_subset = complete_subset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_subset_csv = complete_subset.to_csv('C:/Users/Irisw/OneDrive/Documenten/masterscriptie/complete_subset_csv.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
